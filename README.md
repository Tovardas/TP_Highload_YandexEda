# TP_Highload_YandexEda
## 1. Тема и целевая аудитория
### Тема

Яндекс Еда - сервис заказа еды из кафе, ресторанов, продуктовых магазинов

### Целевая аудитория

 Каждый месяц Яндекс Едой пользуется 15 млн пользователей в более чем 400 городах[^1]. Количество пользователей в день нигде не раскрывается, но на основе данных к концу 2020 года - к началу 2021 с 250 тыс заказами в день и кратным ростом ежемесячной аудитории, можно сделать вывод, что кол-во ежедневных пользователей так же возросло и превысило 1 млн[^2].
 
### Аналоги

- Сбермаркет
- Самокат
- ВкусВилл
 
### Страны, в которых работает сервис

+ Россия 
+ Беларусь 
+ Казахстан 
+ Узбекистан 
+ Армения

### Ключевой функционал

- Регистрация и авторизация
- Просмотр ассортеминта ресторанов/магазинов
- Написание и чтение отзыва с оценкой
- Просмотр рейтинга с выбором источника оценок (Яндекс Еда или Яндекс Карты)
- Поиск
- Определение геопозиции клиента/курьера
- Выкладывать фото, видео и описание блюд/товаров/ресторанов


### Ключевые продуктовые решения

- Отслеживание статуса заказа и местоположения курьера в реальном времени
- Поиск объединяет товары, рестораны и блюда
- Разбиение зоны доставки на зоны с точками питания

## 2. Расчет нагрузки

- MAU - 15 млн [^1]
- DAU - 3 млн (данные, которые были взяты из предположения или примерные величины, полученные опытным путем)
- Среднее время использования приложения за день - 45 минут (учитывая составление заказа + отслеживание курьерской доставки)
- Количество заказов в день - 1 млн 
- За последний год в "Яндекс Еда" регистрировалось более 5 тысяч новых пользователей каждый день
- Коэффициент соотношения пикового трафика к среднему k= 2.3 (рассчитано на основе [^3])
- За 5 минут постоянно листая в среднем получается 500 КБ на все запросы для просмотра ленты (Примерно)
- Пользователь в среденем комментирует каждый 5ый заказ (тк значительная часть пользователей не осталвяют отзывы)

### Продуктовые метрики
#### Рассчет в штуках данных 
Личные данные: 1 запись. (в год)
История заказов: 8 (в неделю)
Платёжные данные: 1 штука. (в год)
Отзывы: 2 (в неделю)

Кол-во ресторанов - 420 [^1]
Кол-во позиций в ресторане - 73

#### Рассчет в байтах данных 

- Личные данные: 1 КБ.
- История заказов: 8 заказов по 1,5 КБ =12 КБ.
- Платёжные данные: 0,5 КБ.
- Отзывы: 2 КБ.

- Размер фото (средний) - 16 Кб
- Размер текстовой информации - 1Кб

Итого за 5 лет:
На одного пользователя:
- Личные данные: 5 записей - 5 КБ
- История заказов: 2080 - 3120 КБ
- Платёжные данные: 5 - 5КБ
- Отзывы: 520 - 520кб
- Сумма: 3650 КБ
На один ресторан(учитывая, что половина позиций меняется за год):
- Фото: 16 * 2,5 * 73 = 2920 КБ
- Текстовая информация: 185 КБ (Аналогично)
- Сумма: 3105 КБ
На всех пользователей:
15 млн * 3650 КБ = 51 ТБ
На все рестораны:
3105 КБ * 420 = 1.3 ТБ
#### Сетевой трафик
Типы трафика(приходящего):
- Запросы на получение списка ресторанов (сортировка, фильтры): каждый запрос примерно 5-10 КБ 
- Запросы на оформление заказов: каждый запрос занимает около 10-15 КБ, включая детали заказа, подтверждение и геоданные
- Запросы на оплату: порядка 5-10 КБ на каждый платёжный запрос
- Отправка push-уведомлений: 0,5-1 КБ на уведомление
Пиковое значение в течение суток (12:00 и 20:00) [^3]
200 тысяч заказов, то есть (10+15+10+1)КБ * 200 тыс = 60 Гигабит/час (примерно) = 
Также нужно учесть пользователей, которые не делают заказ или не оплатили его, это еще около:
200 тыс * 10КБ * 5КБ (нет трафика на оплату, и половина останалвивается на составлении корзины) = 81 Гигабит/час

Тогда суточное потребление составит (280 в пиковые часы + около 500 в остальное время) 780 Гигабит

#### RPS

| Действие                       | Средний RPS         | Пиковое RPS                   |
|--------------------------------|---------------------|-------------------------------|
| Запрос на получение ресторанов | 347                 | 798.1                         | 
| Запрос на получение меню       | 312                 | 717.6                         |
| Запрос на оформление заказов   | 14                  | 32.2                          | 
| Запрос на оплату               | 12                  | 27.6                          | 

## 3. Глобальная балансировка нагрузки
### Географическое расположение дата-центров
![img.png](image.png)

Расположение дата-центров:

На основе этих данных целесообразно расположить дата-центры в крупных городах, чтобы обеспечить минимальное время отклика для пользователей и стабильную работу сервиса.

**Россия:** Дата-центры стоит расположить в крупных городах с распределением по регионам страны:
- Москва (европейская часть России)
- Санкт-Петербург (северо-западная часть России)
- Новосибирск (сибирский регион)
- Владивосток (дальний восток)
  
**Беларусь:**
- Минск (покрытие основной части страны)

**Казахстан:**
- Астана (центр страны)
- Алматы (южная часть Казахстана)

**Узбекистан:**
- Ташкент (основной город для покрытия региона)

**Армения:**
- Ереван (главный центр для обслуживания пользователей в регионе)

#### DNS
Для балансировки трафика на уровне DNS будем использовать Latency-based DNS. Этот механизм позволяет направлять пользователей на ближайший дата-центр с минимальной задержкой (RTT), что обеспечивает высокую производительность и доступность сервиса.

#### BGP
Используем BGP Anycast для присвоения одного IP-адреса нескольким дата-центрам в пределах одной страны. Это позволяет пользователям автоматически направляться к ближайшему по расположению дата-центру, что минимизирует задержки и повышает отказоустойчивость сети.

## 4.Локальная балансировка нагрузки

В нашей схеме балансировки запросы будут поступать на балансировщик уровня L3 (сетевой уровень), который будет быстро распределять трафик между несколькими L7 балансировщиками (уровень приложений). Эти балансировщики L7 будут равномерно распределять запросы по конечным серверам, учитывая особенности запросов, такие как заголовки HTTP, куки или параметры маршрутизации.

#### L3 балансировка

L3 балансировщик распределяет трафик на основе IP-адресов, что позволяет быстро и эффективно перенаправлять запросы на нужные региональные центры обработки данных. Это минимизирует задержки и снижает нагрузку на ближайшие серверы

На сетевом уровне будем использовать Virtual Server с IP Hashing для распределения запросов. Этот метод использует хеширование IP-адреса клиента для того, чтобы направлять запросы на один и тот же сервер при повторных обращениях. Это особенно полезно для поддержания сессий и минимизации переключений между серверами.

Будем использовать Keepalived для мониторинга доступности нод. Keepalived будет следить за состоянием серверов и оповещать балансировщик в случае падения или восстановления ноды, что обеспечит отказоустойчивость системы.

#### L7 балансировка

Будем использовать балансировку с помощью Nginx в роли прокси-сервера. Nginx будет принимать входящие HTTP-запросы от клиентов и распределять их между различными серверами-бэкендами на основе URL или содержимого запросов. Таким образом, обеспечивается равномерное распределение нагрузки, оптимизация производительности и отказоустойчивость системы, что особенно важно при высоких пиковых нагрузках

## 5. Логическая схема БД

![Логическая БД.png](Логическая%20БД%20DZ5.png)

## 6. Физическая схема БД

![Логическая БД.png](Логическая%20БД%20DZ5.png)

postgreSQL

![image](https://github.com/user-attachments/assets/2de1ff18-0653-4307-9de1-7e8004f9c474)

MongoDB

![image](https://github.com/user-attachments/assets/2bee9c38-865d-4a08-976e-06eb81bec489)

Redis

![image](https://github.com/user-attachments/assets/5845b469-c1ae-4821-81c1-07805e6df1f3)

Amazon S3

![image](https://github.com/user-attachments/assets/ec619d4d-113f-48c8-be61-b4c29ebdbd77)

ClickHouse

![image](https://github.com/user-attachments/assets/fdee2992-c163-4f10-9cbf-915273dbd9e6)

### Основные решения
**Выбор СУБД:**
* postgreSQL - для основных сущностей
* MongoDB - для хранения сущностей с geo-данными
* Redis - для таблиц с временными данными и необходимостью быстрого доступа к информации
* Amazon S3 - для хранения фото 
* ClickHouse - для хранения статистики магазинов


**Денормализация:**
* хранение рейтинга  магазина, хотя его можно вычеслить на основе Reviews
* Хранение Категорий в отедльной таблице, вместо enum типа

**Индексы:**
* Уникальный индекс для поля email таблицы users
  * Данный индекс необходим для регистрации/авторизации, когда нужно проверить, есть ли такой email в БД.
* Индекс на поле user_id таблицы Orders:
  * Для быстрого поиска всех заказов, сделанных конкретным пользователем.
* Индекс на поле category_id таблицы Products
  * Для ускорения поиска продуктов по категориям.
* Индекс на поле name таблицы Products
  * Для ускорения поиска продуктов по названию.
* Индекс на поле name таблицы Stores
  * Для ускорения поиска магазинов по названию.
* Индекс на поле store_id таблицы Reviews
  * Для ускорения поиска всех отзывов к одному магазину
* Индекс на поле status таблицы Couriers
  * Для ускорения поиска свобоных курьеров
* Индекс на поле store_id таблицы Products
  * Для ускорения поиска всех продуктов к одному магазину

**Шардирование и резервирование СУБД. Схема резервного копирования:**

Шардинг Корзины по user_id, Отзывы по store_id, Курьеры по delievery_zone, Продукты по store_id, Магазины по delievery_zone, остальные таблицы по id.

В качестве технологии резервного копирования будем использовать полное резервное копирование и инкрементное. Полное резервное копирование будем проводить раз в неделю, а инкрементное будет проходить каждый день. Такая комбинация позволяет минимизировать задержку при сбое основной системы хранения.

## 7. Алгоритмы

**Поиск**

1. Инициализация запросов пользователя

Пользовательский запрос передается на сервер для обработки. На этом этапе система использует техники коррекции ошибок и автодополнения

2. Нормализация и токенизация запроса

Входной запрос разбивается на токены и нормализуется (т.е. приводится к единому формату для обработки синонимов или опечаток)

3. Поиск по индексу

Для быстрого поиска используется индексирвоание данных ресторанов, блюд, магазинов и продуктов. Индексы создаются заранее и обновляются в реальном времени при изменениях (добавление нового контента). Поиск ведется по всем атрибутам, т.е. по названию блюд, по названию ресторанов, по названию магазинов или названию продуктов.

4. Фильтрация по зонам доставки

Для повышения точности поиска система фильтрует рестораны и магазины по зонам доставки. В зависимости от адреса пользователя, подбираются только те заведения, которые находятся в той же зоне.

5. Ранжирование результатов

Далее происходит ранжирование результатов. Есть несколько критериев ранжирования, но у каждого свой вес:

* Популярность
  * Оценки и отзывы
    * Близость к пользователю
 

6. Обработка персонализации

Учитывается история заказов пользователя, что позволяет персонализировать результаты и предлагать те позиции, которые подойдут пользователю с большей вероятностью

7. Кэширование результатов

Результаты поиска кэшируются, что позволяет быстрее отвечать на часто задаваемые запросы без повторной обработки данных

8. Обработка ошибок и отсутствия результатов

В случае, если не найдено точных совпадений, то предлагаются похожие блюда/рестораны или самые популярные места в данной зоне

9. Возврат результатов

Пользователь видит списко релевантных ресторанов и блюд

**Распределение нагрузки по предприятиями**

1. Инициализация

2. Выбор оптимального предприятия

3. Обновление статуса 

**Распределение курьеров по заказам**

1. Получение заказа

2. Проверка доступных курьеров

3. ОЦенка курьеров с помощью алгоритмов распределения

4. Определения траспортного средства

5. Построение маршрута

6. Маршрутизация с несколькими доставками

7. Мониторинг выполнения

8. Завершение заказа

### Источники

[^1]: [Что такое Яндекс Еда](https://dev.go.yandex/services/eda)
[^2]: [Битва за еду: как Delivery Club и «Яндекс.Еда» перекраивают рынок во время пандемии](https://www.forbes.ru/biznes/417203-bitva-za-edu-kak-delivery-club-i-yandekseda-perekraivayut-rynok-vo-vremya-pandemii)
[^3]: [Что заказывают в Яндекс Еде?](https://yandex.ru/company/researches/2020/food)
